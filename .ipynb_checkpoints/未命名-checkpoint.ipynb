{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "anonymous-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-jason",
   "metadata": {},
   "source": [
    "## 爬立委資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ly.gov.tw/Pages/List.aspx?nodeid=109\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = chardet.detect(response.content)['encoding']\n",
    "html_content = response.content.decode(encoding , errors='replace')\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储链接的列表\n",
    "page_lists = []\n",
    "\n",
    "# 查找所有符合条件的链接\n",
    "for a_tag in soup.find_all('a', href=True):\n",
    "    href = a_tag['href']\n",
    "    if \"/Pages/List.aspx?nodeid=\" in href:\n",
    "        page_lists.append(href)\n",
    "\n",
    "# 输出结果\n",
    "for page in page_lists:\n",
    "    print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "link1 = \"https://www.ly.gov.tw\"\n",
    "start = 124\n",
    "end = start + 114\n",
    "\n",
    "# Lists to store the data\n",
    "names = []\n",
    "eng_names = []\n",
    "partys = []\n",
    "sessions = []\n",
    "genders = []\n",
    "photos = []\n",
    "\n",
    "# Directory to save images\n",
    "image_dir = \"legislator_images\"\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "for page in page_lists[start:end]:\n",
    "    url = link1 + page\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract information\n",
    "    name = soup.find('div', class_='legislatorname').get_text(strip=True)\n",
    "    eng_name = soup.find('li', text=lambda x: x and '英文姓名' in x).get_text(strip=True).split('：')[1]\n",
    "    party = soup.find('li', text=lambda x: x and '黨籍' in x).get_text(strip=True).split('：')[1]\n",
    "    session = soup.find('li', text=lambda x: x and '屆別' in x).get_text(strip=True).split('：')[1]\n",
    "    gender = soup.find('li', text=lambda x: x and '性別' in x).get_text(strip=True).split('：')[1]\n",
    "    photo_url = soup.find('img', class_='img-responsive')['src']\n",
    "    \n",
    "    # Store the information\n",
    "    names.append(name)\n",
    "    eng_names.append(eng_name)\n",
    "    partys.append(party)\n",
    "    sessions.append(session)\n",
    "    genders.append(gender)\n",
    "    photos.append(photo_url)\n",
    "    \n",
    "    # Save the photo\n",
    "    \n",
    "    photo_filename = os.path.join(image_dir, photo_url.split('/')[-1])    \n",
    "    with open(photo_filename, 'wb') as f:\n",
    "        f.write(requests.get(link1 + photo_url).content)\n",
    "        print(link1 + photo_url)\n",
    "    \n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': names,\n",
    "    'English Name': eng_names,\n",
    "    'Party': partys,\n",
    "    'Session': sessions,\n",
    "    'Gender': genders,\n",
    "    'Photo Filename': [photo.split('/')[-1] for photo in photos]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('legislators_info.csv', index=False)\n",
    "\n",
    "print(\"Data extraction and image saving complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-rates",
   "metadata": {},
   "source": [
    "## 動態生成分類字典，處理所有出現的黨派和性別組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "heavy-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件，假设包含 Name, English Name, Session, Gender, Photo Filename, Party 列\n",
    "df = pd.read_csv('legislators_info.csv')\n",
    "\n",
    "# 图像目录\n",
    "image_dir = \"legislator_images\"\n",
    "\n",
    "# 初始化分类字典\n",
    "classified_images = {}\n",
    "\n",
    "# 遍历DataFrame，将图像分类\n",
    "for _, row in df.iterrows():\n",
    "    party = row['Party']  # 假设党派信息存在这一列\n",
    "    gender = row['Gender']\n",
    "    photo_filename = row['Photo Filename']\n",
    "    \n",
    "    # 创建分类名称\n",
    "    category = f\"{party}_{gender}\"\n",
    "    \n",
    "    # 如果分类名称不在字典中，则添加它\n",
    "    if category not in classified_images:\n",
    "        classified_images[category] = []\n",
    "    \n",
    "    # 添加图像路径到对应的分类\n",
    "    image_path = os.path.join(image_dir, photo_filename)\n",
    "    if os.path.exists(image_path):\n",
    "        classified_images[category].append(image_path)\n",
    "\n",
    "# 创建保存对齐脸图像的目录\n",
    "aligned_faces_dir = \"aligned_faces\"\n",
    "os.makedirs(aligned_faces_dir, exist_ok=True)\n",
    "\n",
    "# 将每个分类的图像复制到相应的目录\n",
    "for category, images in classified_images.items():\n",
    "    category_dir = os.path.join(aligned_faces_dir, category)\n",
    "    os.makedirs(category_dir, exist_ok=True)\n",
    "    \n",
    "    for image_path in images:\n",
    "        # 复制图像到对应目录\n",
    "        image_name = os.path.basename(image_path)\n",
    "        destination_path = os.path.join(category_dir, image_name)\n",
    "        shutil.copy(image_path, destination_path)  # 使用 shutil.copy 进行复制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-hometown",
   "metadata": {},
   "source": [
    "## 處理68特徵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-purchase",
   "metadata": {},
   "source": [
    "## Delaunay 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-culture",
   "metadata": {},
   "source": [
    "## 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-growth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
